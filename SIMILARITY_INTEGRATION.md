# Similarity Matching Integration

This document describes the integration of the similarity matching functionality into the main pipeline.

## Overview

The similarity matching step (Step 4) compares speech embeddings with sound effect embeddings to find semantically similar sounds for each segment of speech. This enables automatic sound effect selection based on the meaning of spoken words.

## How It Works

### 1. Semantic Embeddings

Both speech segments and sound effects are converted into 384-dimensional embeddings using the `all-MiniLM-L6-v2` model:

- **Speech embeddings**: Generated from transcribed text segments
- **Sound embeddings**: Pre-generated from sound effect titles and descriptions

### 2. Cosine Similarity

For each speech segment, the system:
1. Computes cosine similarity with all available sound effects
2. Ranks sounds by similarity score (0-1, higher is better)
3. Returns the top K most similar sounds

### 3. Output Format

Results are saved as JSON with this structure:

```json
[
  {
    "speech_index": 0,
    "speech_text": "I heard thunder rumbling in the distance",
    "top_matches": [
      {
        "sound_title": "Thunder Rumble",
        "sound_description": "Deep thunder rumbling sound",
        "similarity": 0.8542,
        "audio_url": "https://example.com/thunder.wav"
      },
      {
        "sound_title": "Storm Approaching",
        "sound_description": "Thunder and rain storm",
        "similarity": 0.7234,
        "audio_url": "https://example.com/storm.wav"
      }
    ]
  }
]
```

## Usage

### As Part of Full Pipeline

```bash
# Run complete pipeline with similarity matching
uv run python main.py video.mp4 --full-pipeline
```

This will:
1. Extract audio from video
2. Transcribe with word-level timing
3. Generate speech embeddings
4. Find similar sounds for each segment

### Custom Top-K

Control how many sound matches to find per segment:

```bash
# Find top 10 similar sounds for each segment
uv run python main.py video.mp4 --full-pipeline --top-k 10

# Find top 3 sounds (faster, less options)
uv run python main.py video.mp4 --full-pipeline --top-k 3
```

### Standalone Matching

If you already have embeddings:

```bash
# Just run the matching step
uv run python main.py video.mp4 --run-stt --run-embeddings --run-matching
```

## Module Structure

### Files Modified

1. **`similarity/__init__.py`**
   - Added exports for `find_similar_sounds()` and `cosine_similarity()`

2. **`similarity/similarity.py`**
   - Added `output_path` parameter for configurable output location
   - Improved path handling for flexible output directories

3. **`main.py`**
   - Added `run_semantic_matching_step()` function
   - Integrated similarity matching into pipeline flow
   - Added `--run-matching` and `--top-k` command-line arguments

### New Imports

```python
from similarity import find_similar_sounds
```

## Data Requirements

### Input Files

1. **Speech Embeddings** (generated by pipeline):
   - `data/video_speech_embeddings.csv`
   - Columns: `segment_id`, `text`, `start_time`, `end_time`, `embedding`

2. **Sound Embeddings** (pre-generated):
   - `data/soundbible_embeddings.csv`
   - Columns: `title`, `description`, `audio_url`, `embedding`

### Output Files

- **Similarity Matches**: `output/video_similarity_matches.json`
  - Complete matching results with scores
  - Top K sounds for each speech segment

## Performance

### Timing

For a typical video:
- 10 speech segments
- 500 sound effects database
- Top-5 matching

Matching takes approximately 1-2 seconds on a modern CPU.

### Scaling

The matching is O(n × m) where:
- n = number of speech segments
- m = number of sound effects

For larger databases (1000+ sounds), consider:
- Reducing top-k value
- Pre-filtering sounds by category
- Using approximate nearest neighbor search

## Example Workflow

### Complete Video Processing

```bash
# 1. Run full pipeline
uv run python main.py chaplin_speech.mp4 --full-pipeline --top-k 5

# Output files:
# - speech_to_text/input/chaplin_speech.wav
# - speech_to_text/output/full_transcription.json
# - speech_to_text/output/word_timing.json
# - data/video_speech_embeddings.csv
# - output/video_similarity_matches.json

# 2. Review matches
cat output/video_similarity_matches.json | jq

# 3. Download top matched sounds
# (implement download script based on audio_url)

# 4. Mix audio with video
# (use merging_audio scripts)
```

## Customization

### Different Sound Database

To use a different sound effects database:

1. Generate embeddings for your sounds:
   ```bash
   uv run python utils/sound_embedding/generate_embeddings.py
   ```

2. Ensure CSV has required columns:
   - `embedding`: Vector embedding
   - `title`: Sound name
   - `description`: Sound description
   - `audio_url`: Path or URL to audio file

3. Update the path in main.py:
   ```python
   sound_embeddings_path = Path("data/your_sound_embeddings.csv")
   ```

### Custom Similarity Function

The current implementation uses cosine similarity. To use a different metric:

1. Edit `similarity/similarity.py`
2. Replace `cosine_similarity()` function
3. Options include:
   - Euclidean distance
   - Dot product
   - Custom weighted similarity

## Troubleshooting

### "Sound embeddings not found"

**Error**: `⚠ Sound embeddings not found: data/soundbible_embeddings.csv`

**Solution**: Generate sound embeddings:
```bash
uv run python utils/sound_embedding/generate_embeddings.py
```

### Low Similarity Scores

If all similarity scores are low (<0.3):
- Speech and sound embeddings might be from different domains
- Re-generate embeddings with the same model
- Check that both use `all-MiniLM-L6-v2`

### Memory Issues

For very large sound databases:
- Process in batches
- Reduce top-k value
- Use memory-mapped arrays

## API Reference

### find_similar_sounds()

```python
def find_similar_sounds(
    df_speech: pd.DataFrame,      # Speech embeddings
    df_sounds: pd.DataFrame,       # Sound embeddings
    top_k: int = 5,                # Number of matches
    save_to_json_file: bool = True,
    output_path: str = None        # Output file path
) -> List[Dict[str, Any]]
```

**Parameters:**
- `df_speech`: DataFrame with columns `['text', 'embedding']`
- `df_sounds`: DataFrame with columns `['title', 'description', 'audio_url', 'embedding']`
- `top_k`: Number of top similar sounds to return
- `save_to_json_file`: Whether to save results to JSON
- `output_path`: Custom output path (default: `output/similarity.json`)

**Returns:**
List of dictionaries with matching results

## Future Enhancements

Potential improvements:
1. **Category Filtering**: Match within sound categories
2. **Temporal Awareness**: Consider segment timing and duration
3. **Confidence Thresholds**: Filter low-confidence matches
4. **Audio Feature Matching**: Combine semantic and acoustic similarity
5. **User Feedback Loop**: Learn from manual corrections
6. **Real-time Matching**: Process as segments are generated
7. **Multi-modal Embeddings**: Include visual context from video

## Integration with Audio Mixing

After similarity matching:

1. **Review Matches**: Check `output/video_similarity_matches.json`
2. **Download Sounds**: Use URLs from top matches
3. **Create Timeline**: Convert to audio mixing format
4. **Mix Audio**: Apply sounds at segment timestamps
5. **Export Video**: Combine with original video

See `merging_audio/` directory for audio mixing tools.
